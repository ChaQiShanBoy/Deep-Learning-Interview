{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-Learning-Interview_Part_1\n",
    "\n",
    ">本文原作者Jin Lee，本文来源于知乎专栏。\n",
    "问题集： https://zhuanlan.zhihu.com/p/29936999\n",
    "回答及对应英文页标：https://zhuanlan.zhihu.com/p/29965072\n",
    "elviswf 对上述问题找到 中文版对应页码。github 地址：[那些深度学习《面试》你可能需要知道的（中文页标版）](https://github.com/elviswf/DeepLearningBookQA_cn) https://github.com/elviswf/DeepLearningBookQA_cn\n",
    "\n",
    "----\n",
    "\n",
    "*本人以自身学习，加强基础知识为目的，查漏补缺。将对应页面知识，进行复习，学习，总结提炼。---ZJ*\n",
    "\n",
    "深度学习中文版 2017 年 9 月 4 日版\n",
    "\n",
    "#### 1. 列举常见的一些范数及其应用场景，如 L0，L1，L2，L∞，Frobenius 范数\n",
    "\n",
    "答：书内页面\n",
    "【英】p39-p40 ；还有p230-p236 有 regularization的应用 \n",
    "【中】p34-p35 ；还有 p197-p208 有 regularization 的应用\n",
    "\n",
    " **2.5 范数** \n",
    " \n",
    "有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为 范数（norm）的函数衡量向量大小。形式上，$L^p$ 范数定义如下\n",
    "\n",
    "$$||x||_p = (\\sum_{i}|x_{i}|^p)^{\\frac{1}{p}}\\tag{2.30} $$\n",
    "\n",
    "其中  $p ∈\\mathbb{R}，p ≥ 1$。\n",
    "\n",
    "范数（包括 $L^p$ 范数）是将向量映射到非负值的函数。 直观上来说，向量 $x$ 的范数衡量从原点到点 $x$ 的距离。 更严格地说，范数是满足下列性质的任意函数：\n",
    "\n",
    "- $f(x) = 0 \\Rightarrow x = \\mathbf{0}$\n",
    "- $f(x + y) \\leq f(x) + f(y)$ （三角不等式）\n",
    "- $\\forall \\alpha \\in \\mathbb{R}$, $f(\\alpha x) =\t\\alpha\tf(x)$\n",
    "\n",
    "当$p=2$时，$L^2$范数被称为欧几里得范数。 它表示从原点出发到向量 $x$ 确定的点的欧几里得距离。 $L^2$范数在机器学习中出现地十分频繁，经常简化表示为$||x||$，略去了下标$2$。 平方$L^2$范数也经常用来衡量向量的大小，可以简单地通过点积 $x^Tx$计算。\n",
    "\n",
    "平方$L^2$范数在数学和计算上都比$L^2$范数本身更方便。 例如，平方$L^2$范数对$x$中每个元素的导数只取决于对应的元素，而$L^2$范数对每个元素的导数却和整个向量相关。 但是在很多情况下，平方$L^2$范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。 在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。 在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数：$L^1$范数。 $L^1$范数可以简化如下：\n",
    "\n",
    "$$\\lVert{x}_1\\rVert = \\sum_i  |x_i|.\\tag{2.31}$$\n",
    "\n",
    "\n",
    "当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用 $L^1$ 范数。 每当$x$中某个元素从 $0$ 增加 $\\epsilon$，对应的$L^1$范数也会增加 $\\epsilon$。\n",
    "\n",
    "有时候我们会统计向量中非零元素的个数来衡量向量的大小。 有些作者将这种函数称为”$L^0$范数”，但是这个术语在数学意义上是不对的。 向量的非零元素的数目不是范数，因为对向量缩放$\\alpha$倍不会改变该向量非零元素的数目。 $L^1$范数经常作为表示非零元素数目的替代函数。\n",
    "\n",
    "另外一个经常在机器学习中出现的范数是$L^\\infty$范数，也被称为\\,最大范数。 这个范数表示向量中具有最大幅值的元素的绝对值：\n",
    "\n",
    "$$  \\lVert{x}_1\\rVert _\\infty = \\max_i |x_i|.\\tag{2.32}$$\n",
    "\n",
    "有时候我们可能也希望衡量矩阵的大小。 在深度学习中，最常见的做法是使用 Frobenius 范数，\n",
    "\n",
    " $$\\lVert A \\rVert_F= \\sqrt{\\sum_{i,j} A_{i,j}^2}, $$\n",
    "\n",
    "\n",
    "其类似于向量的$L^2$范数。\n",
    "\n",
    "两个向量的点积可以用范数来表示。 具体地，\n",
    "\n",
    "$$  x^{\\mathrm{T}}y =\\lVert{x}\\rVert_2\\lVert{Y}\\rVert_2 \\cos \\theta\\tag{2.34}$$\n",
    "\n",
    "其中 $\\theta$ 表示 $x$ 和 $y$ 之间的夹角。\n",
    "\n",
    "---\n",
    "\n",
    "**第七章 深度学习中的正则化** （chrome 打开 pdf ----225 ）\n",
    "\n",
    "\n",
    "#### 2. 简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设。\n",
    "\n",
    "答：p35\n",
    "\n",
    "#### 3. 概率密度的万能近似器\n",
    "\n",
    "答：p43：3.10 上面那一段\n",
    "\n",
    "#### 4. 简单介绍一下 sigmoid，relu，softplus，tanh，RBF 及其应用场景\n",
    "\n",
    "答：sigmoid 和 softplus 在 p43 页；全部的在 p123-p127\n",
    "\n",
    "#### 5.Jacobian，Hessian 矩阵及其在深度学习中的重要性\n",
    "\n",
    "答：p56-p62\n",
    "\n",
    "#### 6.KL 散度在信息论中度量的是那个直观量\n",
    "\n",
    "答：p46\n",
    "\n",
    "#### 7. 数值计算中的计算上溢与下溢问题，如 softmax 中的处理方式\n",
    "\n",
    "答：p52-p53\n",
    "\n",
    "#### 8. 与矩阵的特征值相关联的条件数 (病态条件) 指什么，与梯度爆炸与梯度弥散的关系\n",
    "\n",
    "答：p53;\n",
    "\n",
    "#### 9. 在基于梯度的优化问题中，如何判断一个梯度为 0 的零界点为局部极大值／全局极小值还是鞍点，Hessian 矩阵的条件数与梯度下降法的关系\n",
    "\n",
    "答：p56-p62\n",
    "\n",
    "#### 10.KTT 方法与约束优化问题，活跃约束的定义\n",
    "\n",
    "答：p60-p61\n",
    "\n",
    "#### 11. 模型容量，表示容量，有效容量，最优容量概念\n",
    "\n",
    "答：p70;p71;p72\n",
    "\n",
    "#### 12. 正则化中的权重衰减与加入先验知识在某些条件下的等价性\n",
    "\n",
    "答：p74 75\n",
    "\n",
    "#### 13. 高斯分布的广泛应用的缘由\n",
    "\n",
    "答：p40\n",
    "\n",
    "#### 14. 最大似然估计中最小化 KL 散度与最小化分布之间的交叉熵的关系\n",
    "\n",
    "答：p83,84,85\n",
    "\n",
    "#### 15. 在线性回归问题，具有高斯先验权重的 MAP 贝叶斯推断与权重衰减的关系，与正则化的关系\n",
    "\n",
    "答: p87\n",
    "\n",
    "#### 16. 稀疏表示，低维表示，独立表示\n",
    "\n",
    "答：p92\n",
    "\n",
    "#### 17. 列举一些无法基于地图 (梯度？) 的优化来最小化的代价函数及其具有的特点\n",
    "\n",
    "答：p97 维度灾难\n",
    "\n",
    "#### 18. 在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在\n",
    "\n",
    "答：p119-122\n",
    "\n",
    "#### 19. 函数在某个区间的饱和与平滑性对基于梯度的学习的影响\n",
    "\n",
    "答：p98\n",
    "\n",
    "#### 20. 梯度爆炸的一些解决办法 ？？？\n",
    " \n",
    "答：p185 是在这页吗？ 后面再细看\n",
    "\n",
    "#### 21.MLP 的万能近似性质\n",
    "\n",
    "答：p123\n",
    "\n",
    "#### 22. 在前馈网络中，深度与宽度的关系及表示能力的差异 ？？？\n",
    "\n",
    "答：p125\n",
    "\n",
    "#### 23. 为什么交叉熵损失可以提高具有 sigmoid 和 softmax 输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替 sigmoid 的利弊\n",
    "\n",
    "答：p140\n",
    "\n",
    "#### 24. 表示学习的发展的初衷？并介绍其典型例子: 自编码器\n",
    "\n",
    "答：p3\n",
    "\n",
    "许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音鉴别说话者的任务来\n",
    "说，一个有用的特征是对其声道大小的估计。这个特征为判断说话者是男性、女性还是儿童提供了有力线索。\n",
    "\n",
    "然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所以我们可能会想用车轮的存在与否作为特征。不幸的是，我们难以准确地根据像素值来描述车轮看上去像什么。虽然车轮具有简单的几何形状，但它的图像可能会因场景而异，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体等等\n",
    "\n",
    "**解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为表示学习（representation learning）。**学习到的表示往往比手动设计的表示表现得更好。并且它们只需最少的人工干预，就能让AI系统迅速适应新的任务。表示学习算法只需几分钟就可以为简单的任务发现一个很好的特征集，对于复杂任务则需要几小时到几个月。手动为一个复杂的任务设计特征需要耗费大量的人工时间和精力；甚至需要花费整个社群研究人员几十年的时间。\n",
    "\n",
    "**表示学习算法的典型例子是自编码器（autoencoder）。自编码器由一个编码器（encoder）函数和一个解码器（decoder）函数组合而成。**编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 25. 在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚\n",
    "\n",
    "答：p142\n",
    "\n",
    "#### 26. 在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊\n",
    "\n",
    "答：p142\n",
    "\n",
    "#### 27. 正则化过程中，权重衰减与 Hessian 矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系\n",
    "\n",
    "答：p142-144\n",
    "\n",
    "#### 28.L1／L2 正则化与高斯先验／对数先验的 MAP 贝叶斯推断的关系\n",
    "\n",
    "答：p145，146\n",
    "\n",
    "#### 29. 什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛\n",
    "\n",
    "答：p147 页底    `Chapter 7.3`\n",
    "\n",
    "#### 30. 为什么考虑在模型训练时对输入 (隐藏单元／权重) 添加方差较小的噪声，与正则化的关系\n",
    "\n",
    "答：p149-p150    `Chapter 7.5-7.6` \n",
    "\n",
    "#### 31. 共享参数的概念及在深度学习中的广泛影响\n",
    "\n",
    "答：多任务学习 p151;p156     `Chapter 7.7; 7.9`\n",
    "\n",
    "#### 32. Dropout 与 Bagging 集成方法的关系，以及 Dropout 带来的意义与其强大的原因\n",
    "\n",
    "答：p159-p165     `Chapter 7.12` \n",
    "\n",
    "#### 33. 批量梯度下降法更新过程中，批量的大小与各种更新的稳定性关系\n",
    "\n",
    "答：p170    `Chapter 8.1.3`\n",
    "\n",
    "#### 34. 如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散\n",
    "\n",
    "答：p173-p178    `Chapter 8.2.1`\n",
    "\n",
    "#### 35.SGD 以及学习率的选择方法，带动量的 SGD 对于 Hessian 矩阵病态条件及随机梯度方差的影响\n",
    "\n",
    "答：p180；p181-p184    `Chapter 8.3`;\n",
    "\n",
    "#### 36. 初始化权重过程中，权重大小在各种网络结构中的影响，以及一些初始化的方法；偏置的初始化\n",
    "\n",
    "答：初始化权重：p184；  `Chapter 8.4`\n",
    "偏置初始化：p186页底   `Chapter 8.4`\n",
    "\n",
    "#### 37. 自适应学习率算法: AdaGrad，RMSProp，Adam 等算法的做法\n",
    "\n",
    "答：AdaGrad:p187;  \n",
    "RMSProp:p188; \n",
    "Adam:p189       `Chapter 8.5.1-3`\n",
    "\n",
    "#### 38. 二阶近似方法: 牛顿法，共轭梯度，BFGS 等的做法\n",
    "\n",
    "答：牛顿法：p190    `Chapter 8.6.1`; \n",
    "共轭梯度: p191-p193; `Chapter 8.6.2`\n",
    "BFGS:p193-p194    `Chapter 8.6.3`\n",
    "\n",
    "#### 39.Hessian 的标准化对于高阶优化算法的意义\n",
    "\n",
    "答：p195    `Chapter 8.7.1`\n",
    "\n",
    "#### 40. 卷积网络中的平移等变性的原因，常见的一些卷积形式\n",
    "\n",
    "答：平移等变性：p205页底；    `Chapter 9.3`\n",
    "常见的一些卷积形式：p211-p218    `Chapter 9.5`\n",
    "\n",
    "#### 41.pooling 的做法的意义\n",
    "\n",
    "答：p207; p210   `Chapter 9.3-4`\n",
    "\n",
    "#### 42. 循环神经网络常见的一些依赖循环关系，常见的一些输入输出，以及对应的应用场景\n",
    "\n",
    "答：p230-p238    `Chapter 10.2`\n",
    "\n",
    "#### 43. seq2seq，gru，lstm 等相关的原理\n",
    "\n",
    "答：seq2seq:p240-p241;    `Chapter 10.4` \n",
    "gru:p250;    `Chapter 10.10.2`\n",
    "lstm:p248    `Chapter 10.10.1`\n",
    "\n",
    "#### 44. 采样在深度学习中的意义\n",
    "\n",
    "答：p286 第一段    `Chapter 12.4.3` \n",
    "\n",
    "#### 45. 自编码器与线性因子模型，PCA，ICA 等的关系\n",
    "\n",
    "答：线性因子模型可以扩展到自编码器和深度概率模型: p304-p305;  `Chapter 13.5`   \n",
    "PCA:p298;    `Chapter 13.1` \n",
    "ICA:p298    `Chapter 13.2`\n",
    "\n",
    "#### 46. 自编码器在深度学习中的意义，以及一些常见的变形与应用\n",
    "\n",
    "答：意义: p306     `Chapter 14.1` \n",
    "常见变形: p306-p313    `Chapter 14.5` \n",
    "应用: p319   `Chapter 14.9` \n",
    "\n",
    "#### 47. 受限玻尔兹曼机广泛应用的原因\n",
    "\n",
    "答：p400: 想特别了解的人注意这句话：  See Mohamed et al. (2012b) for an analysis of reasons for the success of these models.    `Chapter 20.2` \n",
    "\n",
    "#### 48. 稳定分布与马尔可夫链\n",
    "\n",
    "答：p362    `Chapter 17.3` \n",
    "\n",
    "#### 49.Gibbs 采样的原理\n",
    "\n",
    "答：p365    `Chapter 17.4` \n",
    "\n",
    "#### 50. 配分函数通常难以计算的解决方案\n",
    "\n",
    "答：p368    `Chapter 17.5.2` \n",
    "“遇到难以处理的无向图模型中的配分函数时， 蒙特卡洛方法仍是最主要工具”\n",
    "\n",
    "#### 51. 几种参数估计的联系与区别: MLE／MAP／贝叶斯\n",
    "\n",
    "答：P82/85/87    `Chapter 5.5` \n",
    "\n",
    "#### 52. 半监督的思想以及在深度学习中的应用\n",
    "\n",
    "答：p329-p332    `Chapter 15.3` \n",
    "\n",
    "#### 53. 举例 CNN 中的 channel 在不同数据源中的含义\n",
    "\n",
    "答：p219-220    `Chapter 9.7` \n",
    "\n",
    "#### 54. 深度学习在 NLP，语音，图像等领域的应用及常用的一些模型\n",
    "\n",
    "答：p272-p293    `Chapter 12.1-5` \n",
    "\n",
    "#### 55.word2vec 与 glove 的比较\n",
    "\n",
    "答：How is GloVe different from word2vec?；  \n",
    "\n",
    "GloVe 以及 Word2vec 能称为 deep learning 么？这俩模型的层次其实很浅的；\n",
    "\n",
    "http://t.cn/RvYslDf\n",
    "\n",
    "这个问题没找到答案，我去找了 quora 和知乎上的相关问题以及 quora 一个回答提及的论文。   （若有人在书中找到，请批评指正）\n",
    "\n",
    "#### 56. 注意力机制在深度学习的某些场景中为何会被大量使用，其几种不同的情形\n",
    "\n",
    "答：p288    `Chapter 12.4.5.1` \n",
    "\n",
    "#### 57.wide&deep 模型中的 wide 和 deep 介绍\n",
    "\n",
    "答：https://arxiv.org/pdf/1606.07792.pdf####  此问题答案未在书中找到，为此我去找了原论文，论文图 1 有详细的介绍。 （若有人在书中找到，请批评指正）  \n",
    "\n",
    "#### 58. 核回归与 RBF 网络的关系\n",
    "\n",
    "答：p89    `Chapter 5.7.2` \n",
    "\n",
    "#### 59.LSTM 结构推导，为什么比 RNN 好？\n",
    "\n",
    "答：p248   `Chapter 10.10` \n",
    "\n",
    "#### 60. 过拟合在深度学习中的常见的一些解决方案或结构设计\n",
    "\n",
    "答：p143-159；    `Chapter 7.1-12` \n",
    "包括：Parameter Norm Penalties(参数范数惩罚); Dataset Augmentation (数据集增强); Early Stopping(提前终止);   Parameter Tying and Parameter Sharing (参数绑定与参数共享); Bagging and Other Ensemble Methods(Bagging 和其他集成方法)；Dropout.          另外还有 Batch Normalization。\n",
    "\n",
    "#### 61. 怎么理解贝叶斯模型的有效参数数据会根据数据集的规模自动调整\n",
    "\n",
    "答：关于非参数模型：p72 ；    `Chapter 5.2` \n",
    "非参数模型不依赖于特定的概率模型，它的参数是无穷维的，数据集的规模的大小影响着模型使用更多或者更少的参数来对其进行建模。(并未在书中找到准确的答案，若有更好的回答，请联系我改正)\n",
    "\n",
    "本答案是根据问题在_**Deep Learning**_上找到的答案；有些答案只是自己读书后在书上做的笔记的具体页面，毕竟原 po（http://t.cn/RObdPGk） 说还有另外一本书，所以该答案可能不是特别准确也不完善，答案也是给大家做个参考，若发现答案有问题，请联系我并指正，大家共同进步，谢谢！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
